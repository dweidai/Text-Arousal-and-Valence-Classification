{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('text_emotion.csv', encoding='latin_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content  Unnamed: 2\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...         NaN\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...         NaN\n",
       "2     sadness                Funeral ceremony...gloomy friday...         NaN\n",
       "3  enthusiasm               wants to hang out with friends SOON!         NaN\n",
       "4     neutral  @dannycastillo We want to trade with someone w...         NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df.columns.values.astype('U').tolist()] + df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty', 'sadness', 'sadness', 'enthusiasm', 'neutral', 'worry', 'sadness', 'worry', 'sadness', 'sadness']\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "True\n",
      "40000\n",
      "@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[\n"
     ]
    }
   ],
   "source": [
    "df_sentence = [df_list[x+1][1] for x in range(len(df_list)-1)]\n",
    "emotion = [df_list[x+1][0] for x in range(len(df_list)-1)]\n",
    "print(emotion[0:10])\n",
    "print(type(emotion[0]))\n",
    "print(type('empty'))\n",
    "print(emotion[0] is emotion[526])\n",
    "print(len(emotion))\n",
    "print(df_sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['surprise', 'relief', 'worry', 'anger', 'enthusiasm', 'happiness', 'hate', 'sadness', 'neutral', 'empty', 'fun', 'love', 'boredom']\n"
     ]
    }
   ],
   "source": [
    "mylist = list(set(emotion))\n",
    "'''\n",
    "if emotion[i] is emotion[147]: #relief\n",
    "        valence_3[i] = 1 #for positive valence\n",
    "        arousal_3[i] = 0\n",
    "    if emotion[i] is emotion[4]: #neutral\n",
    "        valence_3[i] = 0 #for positive valence\n",
    "        arousal_3[i] = 0\n",
    "    if emotion[i] is emotion[112]: #boredom\n",
    "        valence_3[i] = 0 #for positive valence\n",
    "        arousal_3[i] = 0\n",
    "    if emotion[i] is emotion[21]: #fun\n",
    "        valence_3[i] = 1 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "    if emotion[i] is emotion[526]: #empty\n",
    "        valence_3[i] = 0 #for positive valence\n",
    "        arousal_3[i] = 0\n",
    "    if emotion[i] is emotion[5]: #worry\n",
    "        valence_3[i] = 0 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "    if emotion[i] is emotion[14]: #surprise\n",
    "        valence_3[i] = 1 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "    if emotion[i] is emotion[16]: #love\n",
    "        valence_3[i] = 1 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "    if emotion[i] is emotion[86]:\n",
    "        valence_3[i] = 0 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "    if emotion[i] is emotion[3]: #enthu\n",
    "        valence_3[i] = 1 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "    if emotion[i] is emotion[1]: #sadness\n",
    "        print(i)\n",
    "        valence_3[i] = 0 #for positive valence\n",
    "        arousal_3[i] = 0\n",
    "    if emotion[i] is emotion[527]:\n",
    "        valence_3[i] = 0 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "    else:\n",
    "        valence_3[i] = 1 #for positive valence\n",
    "        arousal_3[i] = 1\n",
    "        '''\n",
    "print(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "valence_3 = np.zeros(len(emotion))\n",
    "arousal_3 = np.zeros(len(emotion))\n",
    "one_one = []\n",
    "zero_zero = []\n",
    "zero_one = []\n",
    "one_zero = []\n",
    "i = 0\n",
    "j = 0\n",
    "while i < int(len(emotion)):\n",
    "    if(emotion[i] == 'fun' or emotion[i] == 'happiness' or emotion[i] == 'enthusiasm' or emotion[i] == 'love'):\n",
    "        valence_3[j] = 1\n",
    "        arousal_3[j] = 1\n",
    "        one_one.append(df_sentence[i])\n",
    "        j+=1\n",
    "    i += 1\n",
    "i = 0\n",
    "while i < int(len(emotion)):\n",
    "    if(emotion[i] == 'sad' or emotion[i] == 'boredom' or emotion[i] == 'worry'):\n",
    "        valence_3[j] = 0\n",
    "        arousal_3[j] = 0\n",
    "        zero_zero.append(df_sentence[i])\n",
    "        j+=1\n",
    "    i += 1\n",
    "i = 0\n",
    "while i < int(len(emotion)):\n",
    "    if(emotion[i] == 'anger' or emotion[i] == 'hate'):\n",
    "        valence_3[j] = 0\n",
    "        arousal_3[j] = 1\n",
    "        zero_one.append(df_sentence[i])\n",
    "        j+=1\n",
    "    i += 1\n",
    "i = 0\n",
    "while i < int(len(emotion)):\n",
    "    if(emotion[i] == 'neural' or emotion[i] == 'relief'):\n",
    "        valence_3[j] = 1\n",
    "        arousal_3[j] = 0\n",
    "        one_zero.append(df_sentence[i])\n",
    "        j+=1\n",
    "    i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11586\n",
      "1433\n",
      "8638\n",
      "1526\n",
      "40000\n",
      "40000\n",
      "23183\n",
      "23183\n",
      "23183\n"
     ]
    }
   ],
   "source": [
    "print(len(one_one))\n",
    "print(len(zero_one))\n",
    "print(len(zero_zero))\n",
    "print(len(one_zero))\n",
    "print(len(valence_3))\n",
    "print(len(arousal_3))\n",
    "valence3 = np.zeros(j)\n",
    "arousal3 = np.zeros(j)\n",
    "for i in range(j):\n",
    "    valence3[i] = valence_3[i]\n",
    "    arousal3[i] = arousal_3[i]\n",
    "print(len(valence3))\n",
    "print(len(arousal3))\n",
    "df_sentence = one_one + zero_zero + zero_one + one_zero\n",
    "print(len(df_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wants to hang out with friends SOON!\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(df_sentence[0])\n",
    "print(valence3[0])\n",
    "print(arousal3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = pd.read_csv('emo.csv', encoding='latin_1')\n",
    "emo.dropna()\n",
    "#fb = pd.read_csv('fbemo.csv')\n",
    "#fb.dropna()\n",
    "#fb_list = [fb.columns.values.astype('U').tolist()] + fb.values.tolist()\n",
    "emo_list = [emo.columns.values.astype('U').tolist()] + emo.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10062,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "valence_1 = np.zeros(len(emo_list)-1)\n",
    "arousal_1 = np.zeros(len(emo_list)-1)\n",
    "i = 1\n",
    "while i < int(len(emo_list)):\n",
    "    if emo_list[i][1] >= 3:\n",
    "        valence_1[i-1] = 1 #for positive valence\n",
    "    else:\n",
    "        valence_1[i-1] = 0 #for negative valence\n",
    "    if emo_list[i][2] >= 3:\n",
    "        arousal_1[i-1] = 1 #for positive arousal\n",
    "    else:\n",
    "        arousal_1[i-1] = 0 #for negative arousal\n",
    "    i+=1\n",
    "'''\n",
    "valence_2 = np.zeros(len(fb_list)-1)\n",
    "arousal_2 = np.zeros(len(fb_list)-1)\n",
    "i = 1\n",
    "while i < int(len(fb_list)):\n",
    "    if (fb_list[i][1] + fb_list[i][2])/2 >= 5:\n",
    "        valence_2[i-1] = 1 #for positive valence\n",
    "    else:\n",
    "        valence_2[i-1] = 0 #for negative valence\n",
    "    if (fb_list[i][3] + fb_list[i][4])/2 >= 5:\n",
    "        arousal_2[i-1] = 1 #for positive arousal\n",
    "    else:\n",
    "        arousal_2[i-1] = 0 #for negative arousal\n",
    "    i+=1\n",
    "'''\n",
    "print(valence_1.shape)\n",
    "#print(valence_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_sentence = [emo_list[x+1][0] for x in range(len(emo_list)-1)]\n",
    "#fb_sentence = [fb_list[x+1][0] for x in range(len(fb_list)-1)]\n",
    "#sentence = emo_sentence + fb_sentence + df_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = emo_sentence + df_sentence\n",
    "valence_np = np.hstack((valence_1, valence3)) #, valence_2\n",
    "arousal_np = np.hstack((arousal_1, arousal3)) #, arousal_2\n",
    "valence = valence_np.tolist()\n",
    "arousal = arousal_np.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33245\n",
      "33245\n",
      "33245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#S_train = [sentence[i].value().astype(U) for i in range(len(sentence))]\n",
    "S_train = sentence\n",
    "v_train = valence \n",
    "a_train = arousal\n",
    "print(len(S_train))\n",
    "print(len(v_train))\n",
    "print(len(a_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "middle_words = ['and','a','the','am','it','me','with','in','on','by','near','this','that','an','there','here','those','ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once',\n",
    "                'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into',\n",
    "                'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves',\n",
    "                'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', \n",
    "                'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', \n",
    "                'all', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves',\n",
    "                'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'myself', 'which', 'those', 'i',\n",
    "                'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "middle_words = set(dict.fromkeys([stemmer.stem(word) for word in middle_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    for w in middle_words:\n",
    "        while w in tokens:\n",
    "            tokens.remove(w)\n",
    "    toReturn = [stemmer.stem(item.lower()) for item in tokens]\n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    middle_words = ['and','a','the','am','it','me','with','in','on','by','near','this','that','an','there','here','those',\n",
    "                'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once','during', 'out', 'very',\n",
    "                'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most',\n",
    "                'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves',\n",
    "                'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more',\n",
    "                'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she',\n",
    "                'all', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does',\n",
    "                'yourselves', 'then', 'that', 'because', 'what', 'over', 'so', 'can', 'did', 'now', 'under', 'he', 'you',\n",
    "                'herself', 'has', 'just', 'where', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if',\n",
    "                'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than', 's', 't', 'can', 'will',\n",
    "                'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren']\n",
    "    middle_words = set(dict.fromkeys([stemmer.stem(word) for word in middle_words]))\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    for w in middle_words:\n",
    "        while w in tokens:\n",
    "            tokens.remove(w)\n",
    "    toReturn = [stemmer.stem(item.lower()) for item in tokens]\n",
    "    return toReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vect = TfidfVectorizer(ngram_range=(1,3), tokenizer=tokenize)\n",
    "trainX = count_vect.fit_transform(S_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(v_train)\n",
    "target_labels = le.classes_\n",
    "trainy = le.transform(v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random(X,y):\n",
    "    from sklearn.linear_model import RandomizedLogisticRegression\n",
    "    randomized_logistic = RandomizedLogisticRegression()\n",
    "    randomized_logistic.fit(X, y)\n",
    "    print(\"Parameters: \", randomized_logistic.get_params)\n",
    "    print(\"Score: \", str(randomized_logistic.score(X,y)))\n",
    "    return randomized_logistic\n",
    "\n",
    "def train_bagging(X,y):\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    bagging = BaggingClassifier(base_estimator = LogisticRegression(C=5, random_state=0, solver='lbfgs',class_weight = 'balanced', max_iter=10000))\n",
    "    f_list = [0.25,0.5,0.75]\n",
    "    parameters_bagging = {'max_features': f_list}\n",
    "    grid = GridSearchCV(bagging, parameters_bagging, cv=5)\n",
    "    grid.fit(X, y)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "    print(\"Best parameters: \", grid.best_params_)\n",
    "    print(\"Best estimator: \", grid.best_estimator_)\n",
    "    cls_bagging = grid.best_estimator_\n",
    "    cls_bagging.fit(X, y)\n",
    "    return cls_bagging\n",
    "    \n",
    "def train_classifier(X, y):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {'C': [1, 5, 10, 25]}\n",
    "    print(\"grid search start\")\n",
    "    grid = GridSearchCV(LogisticRegression(random_state=0, solver='lbfgs',class_weight = 'balanced', max_iter=10000), param_grid, cv=5)\n",
    "    print(\"done grid search\")\n",
    "    grid.fit(X, y)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "    print(\"Best parameters: \", grid.best_params_)\n",
    "    print(\"Best estimator: \", grid.best_estimator_)\n",
    "    cls = grid.best_estimator_\n",
    "    cls.fit(X, y)\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search start\n",
      "done grid search\n",
      "Best cross-validation score: 0.73\n",
      "Best parameters:  {'C': 5}\n",
      "Best estimator:  LogisticRegression(C=5, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=0,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "cls_valence = train_classifier(trainX, trainy)\n",
    "#svc_valence = train_svc(trainX, trainy)\n",
    "#bagging_valence = train_bagging(trainX, trainy)\n",
    "#cls_random = train_random_forest(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_a = preprocessing.LabelEncoder()\n",
    "le_a.fit(a_train)\n",
    "target_labels_a = le_a.classes_\n",
    "trainy = le_a.transform(a_train)\n",
    "le_a = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search start\n",
      "done grid search\n",
      "Best cross-validation score: 0.64\n",
      "Best parameters:  {'C': 10}\n",
      "Best estimator:  LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=0,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "cls_arousal = train_classifier(trainX, trainy)\n",
    "#bagging_arousal = train_bagging(trainX, trainy)\n",
    "#random_arousal = train_random(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c7a4fcebfc37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I am more ready than ever!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlr_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_valence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_arousal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_vect' is not defined"
     ]
    }
   ],
   "source": [
    "test_list = ['I am more ready than ever!']\n",
    "test = count_vect.transform(test_list)\n",
    "lr_v = cls_valence.predict(test)\n",
    "lr_a = cls_arousal.predict(test)\n",
    "print(lr_v)\n",
    "print(lr_a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dc1e3f979262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_v\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlr_a\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpil_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'happy.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are Happy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_v' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "if(lr_v == 1 and lr_a == 1):\n",
    "    pil_im = Image.open('happy.jpeg', 'r')\n",
    "    print(\"You are Happy\")\n",
    "elif(lr_v == 1 and lr_a == 0):\n",
    "    pil_im = Image.open('chilling.jpeg', 'r')\n",
    "    print(\"You are just Chilling\")\n",
    "elif(lr_v == 0 and lr_a == 1):\n",
    "    pil_im = Image.open('angry.jpeg', 'r')\n",
    "    print(\" You are really displeased\")\n",
    "elif(lr_v == 0 and lr_a == 0):\n",
    "    pil_im = Image.open('sad.jpeg', 'r')\n",
    "    print(\"You are bored or you are sad\")\n",
    "imshow(np.asarray(pil_im))\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
